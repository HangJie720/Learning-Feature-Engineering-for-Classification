{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, regularizers, Flatten\n",
    "\n",
    "from sklearn import ensemble, preprocessing, multiclass\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transformation\n",
    "\n",
    "def sqrt(col):\n",
    "    return list(map(np.sqrt, col));\n",
    "\n",
    "def freq(col):\n",
    "    col = np.floor(col)\n",
    "    counter = Counter(col)\n",
    "    return [counter.get(elem) for elem in col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "# Datasets\n",
    "dids = np.load(\"datasets/indexes.npy\")\n",
    "\n",
    "# RF model parameters\n",
    "seed = 67\n",
    "transformations = [sqrt, freq]\n",
    "transformations_name = [\"sqrt\", \"freq\"]\n",
    "trans2target = {}\n",
    "\n",
    "# Comrpessed Dataset paramters\n",
    "qsa_representation = []\n",
    "num_bin = 200\n",
    "too_big = 100000\n",
    "\n",
    "# Neural Nets Parameters and Variables\n",
    "MLP_LFE_Nets = {}\n",
    "inp_shape = (2,num_bin)\n",
    "dropout = 0.2\n",
    "norm = (-10, 10)\n",
    "pred_threshold = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def binarize_dataset():\n",
    "\n",
    "def load_dataset(id):\n",
    "    X = np.load(\"datasets/\" + str(id) + \"-data.npy\")\n",
    "    y = np.load(\"datasets/\" + str(id) + \"-target.npy\")\n",
    "    categorical = np.load(\"datasets/\" + str(id) + \"-categorical.npy\")\n",
    "    return X,y,categorical\n",
    "\n",
    "    \n",
    "def evaluate_model(X, y, categorical):\n",
    "    imp = Imputer(missing_values=\"NaN\")\n",
    "    X = imp.fit_transform(X)\n",
    "    enc = preprocessing.OneHotEncoder(categorical_features=categorical)\n",
    "    X = enc.fit_transform(X)\n",
    "    clf = ensemble.RandomForestClassifier(random_state=seed)\n",
    "    #clf_ovsr = multiclass.OneVsRestClassifier(clf, n_jobs=-1)\n",
    "    \n",
    "    return cross_val_score(clf, X, y,cv=10)\n",
    "    \n",
    "def is_positive(X,y,categorical,base_score,transformation,feature):\n",
    "    transformed_feature = np.array(transformation(X[:,feature]))\n",
    "    X = np.c_[X,transformed_feature]\n",
    "    categorical = np.append(categorical,False)\n",
    "    new_score = evaluate_model(X,y,categorical).mean()\n",
    "    \n",
    "    return 1 if(base_score <= (new_score - 0.01)) else 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the target for the compressed feature\n",
    "bad_datasets = []\n",
    "\n",
    "def build_target_for_compressed(dids):\n",
    "\n",
    "    for transf in transformations:\n",
    "        trans2target[transf] = []\n",
    "\n",
    "    \n",
    "    for did in dids:\n",
    "        print(\"Start dataset number\", did)\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            X, y, categorical = load_dataset(did)       \n",
    "\n",
    "            new_indexes = []\n",
    "\n",
    "            if(X.shape[0] > too_big):\n",
    "                new_indexes = np.random.choice(X.shape[0], too_big, replace=False)\n",
    "                X = X[new_indexes]\n",
    "                y = y[new_indexes]\n",
    "\n",
    "            base_score = evaluate_model(X, y, categorical).mean()\n",
    "\n",
    "            # Find the indexes of numeric attributes\n",
    "            numerical_indexes = np.where(np.invert(categorical))[0]\n",
    "\n",
    "            for i,transf in enumerate(transformations):\n",
    "                for feature in numerical_indexes:\n",
    "\n",
    "                    print(\"\\tEvaluating feature \" + str(feature))\n",
    "\n",
    "                    mlp_target = is_positive(X,y,categorical,base_score,transf, feature)\n",
    "\n",
    "                    print(\"\\t\\t\" + str(mlp_target))\n",
    "\n",
    "                    trans2target[transf].append((did,feature,mlp_target))\n",
    "        except:\n",
    "                print(\"The evaluation of dataset \" + str(did) + \" failed\")\n",
    "                bad_datasets.append(did)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the result\n",
    "def save_target_for_compressed(path):\n",
    "\n",
    "    for transf, name in zip(transformations, transformations_name):\n",
    "        np.save(path + name, trans2target[transf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_Rx(matrix):\n",
    "    \n",
    "    Rxc = np.zeros(shape=matrix.shape)\n",
    "    \n",
    "    for i,row in enumerate(matrix):\n",
    "        max_c = np.amax(row)\n",
    "        min_c = np.amin(row)\n",
    "        bin_width = (max_c-min_c)/(norm[1]-norm[0])\n",
    "        Rxc[i] = np.apply_along_axis(lambda x : np.floor((x-min_c)/(bin_width)+norm[0]), 0, row)\n",
    "    \n",
    "    return Rxc\n",
    "\n",
    "def to_quantile_sketch_array(did, col, targets, bins, t_class, index):\n",
    "    max_c = np.nanmax(col)\n",
    "    min_c = np.nanmin(col)\n",
    "    bin_width = (max_c-min_c)/num_bin\n",
    "    Rx = np.zeros(shape=(2,num_bin))\n",
    "    \n",
    "    if(bin_width == 0):\n",
    "        return\n",
    "    \n",
    "    for val,y in zip(col,targets):\n",
    "        if not np.isnan(val):\n",
    "            bin_value = int(np.floor((val-min_c)/bin_width))\n",
    "            bin_value = np.clip(bin_value, 0, num_bin-1)\n",
    "            my_class = 0 if t_class == y else 1\n",
    "            Rx[my_class][bin_value] = Rx[my_class][bin_value] + 1\n",
    "            \n",
    "    Rx = normalize_Rx(Rx)\n",
    "\n",
    "    qsa_representation.append(np.insert(Rx.flatten(), 0, [did,index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the compressed dataset\n",
    "def build_compressed_dataset(dids):\n",
    "\n",
    "    qsa_representation = []\n",
    "\n",
    "    for did in dids[:2]:\n",
    "        print(\"Start dataset number\", did)\n",
    "\n",
    "        try:\n",
    "            X, y, categorical = load_dataset(did)\n",
    "        except:\n",
    "            print(\"Dataset \" + str(did) + \" not found\")\n",
    "            continue;\n",
    "\n",
    "        new_indexes = []\n",
    "\n",
    "        if(X.shape[0] > too_big):\n",
    "            new_indexes = np.random.choice(X.shape[0], too_big, replace=False)\n",
    "            X = X[new_indexes]\n",
    "            y = y[new_indexes]\n",
    "\n",
    "        numerical_indexes = np.where(np.invert(categorical))[0]\n",
    "\n",
    "        classes = set(y)\n",
    "\n",
    "        for t_class in classes:\n",
    "            for index in numerical_indexes:\n",
    "                to_quantile_sketch_array(did,X[:,index], y, num_bin, t_class, index)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the compressed datasets\n",
    "def save_compressed_dataset(path):\n",
    "    np.save(path + \"compressed.npy\", qsa_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CREATING THE NEURAL NETS\n",
    "\n",
    "def initialize_MLPs():\n",
    "    \n",
    "    for transf in transformations_name:\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, input_shape=inp_shape, W_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.add(Dense(64, W_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(output_dim=1))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "        # For a binary classification problem\n",
    "        model.compile(optimizer='rmsprop',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        MLP_LFE_Nets[transf] = model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_compressed_ds():\n",
    "    data = np.load(\"datasets/compressed/compressed.npy\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def assemble_training_set(compressed, transformation_targets):\n",
    "    targetDf = pd.DataFrame(transformation_targets)\n",
    "    merged = pd.merge(compressed, targetDf, how='left', on=[0, 1])\n",
    "    class_1 = merged.ix[:,4:num_bin + 4].values\n",
    "    class_2 = merged.ix[:,num_bin + 4:-1].values\n",
    "    target = np.array(merged.ix[:,-1].values)\n",
    "    meta_inf = np.array(merged.ix[:,:2].values)\n",
    "    meta_target = np.c_[target, meta_inf]\n",
    "    X = []\n",
    "    \n",
    "    for c1, c2 in zip(class_1, class_2):\n",
    "        X.append([c1,c2])    \n",
    "\n",
    "    return np.array(X), np.array(meta_target)\n",
    "\n",
    "\n",
    "def split_training_test():\n",
    "    compressed_ds = load_compressed_ds()\n",
    "\n",
    "    for transf, name in zip(transformations, transformations_name):\n",
    "        transformation_targets = np.load(\"datasets/compressed/\" + name + \".npy\")\n",
    "        X,y = assemble_training_set(compressed_ds, transformation_targets)\n",
    "        \n",
    "        X_s_tr, X_s_test, y_s_tr, y_s_test = train_test_split(X, y, test_size=0.2)\n",
    "        \n",
    "        # Dropping the meta-info from training set\n",
    "        y_s_tr = y_s_tr[:,:1]\n",
    "        \n",
    "        np.save(\"datasets/training/\" + name + \"-data_split\",X_s_tr)\n",
    "        np.save(\"datasets/training/\" + name + \"-target_split\",y_s_tr)\n",
    "        np.save(\"datasets/test/\" + name + \"-data_split\",X_s_test)\n",
    "        np.save(\"datasets/test/\" + name + \"-target_split\",y_s_test)\n",
    "\n",
    "        \n",
    "def load_training_set(transf):\n",
    "    \n",
    "    X = np.load(\"datasets/training/\" + transf + \"-data_split.npy\")\n",
    "    y = np.load(\"datasets/training/\" + transf + \"-target_split.npy\")\n",
    "    \n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the nets\n",
    "\n",
    "def train_MLPs():\n",
    "\n",
    "    for transf, name in zip(transformations, transformations_name):\n",
    "        \n",
    "        X,y = load_training_set(name)\n",
    "\n",
    "        MLP_LFE_Nets[name].summary()\n",
    "        print (\"Inputs: {}\".format(MLP_LFE_Nets[name].input_shape))\n",
    "        print (\"Outputs: {}\".format(MLP_LFE_Nets[name].output_shape))\n",
    "        print (\"Actual input: {}\".format(X.shape))\n",
    "        print (\"Actual output: {}\".format(y.shape))   \n",
    "\n",
    "        MLP_LFE_Nets[name].fit(X, y)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_MLPs():\n",
    "    for transf in transformations_name:\n",
    "        MLP_LFE_Nets[transf].save_weights(\"datasets/MLPs/\" + transf + \"-weights\")\n",
    "        model_json = MLP_LFE_Nets[transf].to_json()\n",
    "        with open(\"datasets/MLPs/\" + transf + \"-net_model\", \"w\") as f:\n",
    "            f.write(model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_set():\n",
    "    \n",
    "    X = np.load(\"datasets/test/\" + transformations_name[0] + \"-data_split.npy\")\n",
    "    y_meta = np.load(\"datasets/test/\" + transformations_name[0] + \"-target_split.npy\")\n",
    "    t = np.full((y_meta.shape[0],1), 0)\n",
    "    y_meta = np.concatenate((y_meta, t), axis=1)\n",
    "    \n",
    "    for i,name in enumerate(transformations_name[1:]):\n",
    "        X = np.concatenate((X, np.load(\"datasets/test/\" + name + \"-data_split.npy\")), axis=0)\n",
    "        y_meta_tmp = np.load(\"datasets/test/\" + name + \"-target_split.npy\")\n",
    "        t = np.full((y_meta_tmp.shape[0],1),i+1)\n",
    "        y_meta_tmp = np.concatenate((y_meta_tmp, t), axis=1)\n",
    "        y_meta = np.concatenate((y_meta, y_meta_tmp), axis=0)\n",
    "    \n",
    "    return X,y_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the accuracy on a dataset\n",
    "\n",
    "def evaluate_transformation_classifier():\n",
    "    \n",
    "    # Number of prediction on features\n",
    "    num_of_prediction = 0\n",
    "    # Number of correct prediction on features\n",
    "    num_of_correct_prediction = 0\n",
    "    # Number of dataset which received a prediction\n",
    "    good_predicted_dids = set()\n",
    "    num_of_predicted_dataset = 0\n",
    "    \n",
    "    pred_mat = []\n",
    "    \n",
    "    X, y_meta = load_test_set()\n",
    "    \n",
    "    for transf in transformations_name:\n",
    "        pred_mat.append(MLP_LFE_Nets[transf].predict(X))\n",
    "        \n",
    "    pred_mat = np.array(pred_mat).transpose()\n",
    "    \n",
    "    for predictions,did,feature in zip(pred_mat,y_meta[:,1], y_meta[:,2]):\n",
    "        pmax = np.amax(predictions)\n",
    "        \n",
    "        if pmax > pred_threshold:\n",
    "            index = np.where(predictions==pmax)[0][0]\n",
    "            num_of_prediction += 1\n",
    "            \n",
    "            # Select the target for the transformation and the dataset\n",
    "            positive_example_found = len(np.where((y_meta[:,0] == 1)       &  \\\n",
    "                                                  (y_meta[:,1] == did)     &  \\\n",
    "                                                  (y_meta[:,2] == feature) &  \\\n",
    "                                                  (y_meta[:,3] == index)))    \\\n",
    "                                                                                  >0\n",
    "            \n",
    "            if(positive_example_found):\n",
    "                good_predicted_dids.add(did) \n",
    "                num_of_correct_prediction += 1\n",
    "    \n",
    "    print(\"Number of prediction:\", num_of_prediction)\n",
    "    print(\"Number of Correct prediciton:\", num_of_correct_prediction)\n",
    "    print(\"Accuracy:\", num_of_correct_prediction/num_of_prediction)\n",
    "    print(\"Number of datasets who received a good prediction:\", len(good_predicted_dids))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "build_target_for_compressed(dids)\n",
    "save_target_for_compressed(\"datasets/compressed/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "build_compressed_dataset(dids)\n",
    "save_compressed_dataset(\"datasets/compressed/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "split_training_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Train the MLPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_19 (Dense)                 (None, 2, 64)         12864       dense_input_7[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 2, 64)         0           dense_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 2, 64)         4160        activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 2, 64)         0           dense_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 128)           0           dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 1)             129         flatten_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 1)             0           dense_21[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 17,153\n",
      "Trainable params: 17,153\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Inputs: (None, 2, 200)\n",
      "Outputs: (None, 1)\n",
      "Actual input: (218, 2, 200)\n",
      "Actual output: (218, 1)\n",
      "Epoch 1/10\n",
      "218/218 [==============================] - 3s - loss: 1.4636 - acc: 1.0000     \n",
      "Epoch 2/10\n",
      "218/218 [==============================] - 0s - loss: 1.2109 - acc: 1.0000     \n",
      "Epoch 3/10\n",
      "218/218 [==============================] - 0s - loss: 1.0551 - acc: 1.0000     \n",
      "Epoch 4/10\n",
      "218/218 [==============================] - 0s - loss: 0.9251 - acc: 1.0000     \n",
      "Epoch 5/10\n",
      "218/218 [==============================] - 0s - loss: 0.8102 - acc: 1.0000     \n",
      "Epoch 6/10\n",
      "218/218 [==============================] - 0s - loss: 0.7071 - acc: 1.0000     \n",
      "Epoch 7/10\n",
      "218/218 [==============================] - 0s - loss: 0.6137 - acc: 1.0000     \n",
      "Epoch 8/10\n",
      "218/218 [==============================] - 0s - loss: 0.5291 - acc: 1.0000     \n",
      "Epoch 9/10\n",
      "218/218 [==============================] - 0s - loss: 0.4529 - acc: 1.0000     \n",
      "Epoch 10/10\n",
      "218/218 [==============================] - 0s - loss: 0.3845 - acc: 1.0000     \n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_22 (Dense)                 (None, 2, 64)         12864       dense_input_8[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 2, 64)         0           dense_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (None, 2, 64)         4160        activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 2, 64)         0           dense_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 128)           0           dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 1)             129         flatten_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 1)             0           dense_24[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 17,153\n",
      "Trainable params: 17,153\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Inputs: (None, 2, 200)\n",
      "Outputs: (None, 1)\n",
      "Actual input: (218, 2, 200)\n",
      "Actual output: (218, 1)\n",
      "Epoch 1/10\n",
      "218/218 [==============================] - 0s - loss: 3.3217 - acc: 0.3578     \n",
      "Epoch 2/10\n",
      "218/218 [==============================] - 0s - loss: 1.8778 - acc: 0.6743     \n",
      "Epoch 3/10\n",
      "218/218 [==============================] - 0s - loss: 1.6746 - acc: 0.6972     \n",
      "Epoch 4/10\n",
      "218/218 [==============================] - 0s - loss: 1.5042 - acc: 0.7385     \n",
      "Epoch 5/10\n",
      "218/218 [==============================] - 0s - loss: 1.4249 - acc: 0.7477     \n",
      "Epoch 6/10\n",
      "218/218 [==============================] - 0s - loss: 1.3065 - acc: 0.7385     \n",
      "Epoch 7/10\n",
      "218/218 [==============================] - 0s - loss: 1.2138 - acc: 0.7477     \n",
      "Epoch 8/10\n",
      "218/218 [==============================] - 0s - loss: 1.1336 - acc: 0.7385     \n",
      "Epoch 9/10\n",
      "218/218 [==============================] - 0s - loss: 1.0522 - acc: 0.7477     \n",
      "Epoch 10/10\n",
      "218/218 [==============================] - 0s - loss: 0.9799 - acc: 0.7385     \n"
     ]
    }
   ],
   "source": [
    "\n",
    "initialize_MLPs()\n",
    "train_MLPs()\n",
    "save_MLPs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Test the nets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prediction: 1\n",
      "Number of Correct prediciton: 1\n",
      "Accuracy: 1.0\n",
      "Number of datasets who received a good prediciton: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluate_transformation_classifier()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
